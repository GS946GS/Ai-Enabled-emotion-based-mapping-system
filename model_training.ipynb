{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a69d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "c:\\Users\\sinth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sinth\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sinth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Training FOLD 1 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1:   0%|          | 2/8334 [00:03<3:03:05,  1.32s/it]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [36:18<00:00,  3.83it/s]  \n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1 EPOCH 1 â†’ F1: 0.8412 | Acc: 0.8464\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2:   0%|          | 1/8334 [00:00<33:20,  4.17it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [35:29<00:00,  3.91it/s] \n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1 EPOCH 2 â†’ F1: 0.9533 | Acc: 0.9534\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3:   0%|          | 1/8334 [00:00<35:55,  3.87it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [32:05<00:00,  4.33it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1 EPOCH 3 â†’ F1: 0.9870 | Acc: 0.9871\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4:   0%|          | 3/8334 [00:00<32:28,  4.28it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:45<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1 EPOCH 4 â†’ F1: 0.9882 | Acc: 0.9882\n",
      "ðŸ”¥ Best model updated!\n",
      "Best F1 for Fold 1: 0.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sinth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Training FOLD 2 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1:   0%|          | 2/8334 [00:00<33:18,  4.17it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:44<00:00,  4.38it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 2 EPOCH 1 â†’ F1: 0.6826 | Acc: 0.7053\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2:   0%|          | 1/8334 [00:00<32:24,  4.29it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:45<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 2 EPOCH 2 â†’ F1: 0.9294 | Acc: 0.9309\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3:   0%|          | 1/8334 [00:00<32:55,  4.22it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:46<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 2 EPOCH 3 â†’ F1: 0.9686 | Acc: 0.9685\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4:   0%|          | 1/8334 [00:00<31:54,  4.35it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:45<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 2 EPOCH 4 â†’ F1: 0.9764 | Acc: 0.9762\n",
      "ðŸ”¥ Best model updated!\n",
      "Best F1 for Fold 2: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sinth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Training FOLD 3 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1:   0%|          | 2/8334 [00:00<32:36,  4.26it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:46<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 3 EPOCH 1 â†’ F1: 0.6852 | Acc: 0.7035\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2:   0%|          | 1/8334 [00:00<32:14,  4.31it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:45<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 3 EPOCH 2 â†’ F1: 0.9313 | Acc: 0.9324\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3:   0%|          | 1/8334 [00:00<32:07,  4.32it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:45<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 3 EPOCH 3 â†’ F1: 0.9685 | Acc: 0.9687\n",
      "ðŸ”¥ Best model updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/8334 [00:00<?, ?it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:208: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4:   0%|          | 2/8334 [00:00<33:28,  4.15it/s]C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8334/8334 [31:47<00:00,  4.37it/s]\n",
      "C:\\Users\\sinth\\AppData\\Local\\Temp\\ipykernel_20044\\2257095105.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 3 EPOCH 4 â†’ F1: 0.9765 | Acc: 0.9767\n",
      "ðŸ”¥ Best model updated!\n",
      "Best F1 for Fold 3: 0.9765\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# 1. GENERAL CONFIG & SEEDING\n",
    "# =========================================================\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================================================\n",
    "# 2. LOAD DATASET\n",
    "# =========================================================\n",
    "\n",
    "df = pd.read_csv(\"emotion_master_200k_balanced.csv\")\n",
    "\n",
    "# Use only emotion as target (training first model)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"emotion\"].tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "df[\"label\"] = labels\n",
    "\n",
    "# =========================================================\n",
    "# 3. GENERALIZED NLP AUGMENTATIONS\n",
    "# =========================================================\n",
    "\n",
    "def random_mask(text, p=0.15):\n",
    "    \"\"\"Randomly mask some tokens\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 4:\n",
    "        return text\n",
    "    for i in range(len(words)):\n",
    "        if random.random() < p:\n",
    "            words[i] = \"<mask>\"\n",
    "    return \" \".join(words)\n",
    "\n",
    "def word_dropout(text, p=0.10):\n",
    "    \"\"\"Randomly drop some words\"\"\"\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if random.random() > p:\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words) if new_words else text\n",
    "\n",
    "def random_swap(text):\n",
    "    \"\"\"Swap two words randomly\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return text\n",
    "    idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def augment(text):\n",
    "    \"\"\"Apply multiple augmentation layers\"\"\"\n",
    "    if random.random() < 0.30:\n",
    "        text = random_mask(text)\n",
    "    if random.random() < 0.30:\n",
    "        text = word_dropout(text)\n",
    "    if random.random() < 0.20:\n",
    "        text = random_swap(text)\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# 4. DATASET CLASS\n",
    "# =========================================================\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, augment_data=False):\n",
    "        self.df = df\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "        label = self.df.iloc[idx][\"label\"]\n",
    "\n",
    "        if self.augment_data:\n",
    "            text = augment(text)\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            max_length=96,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"text_embedding\": enc[\"input_ids\"]  # used for mixup\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# 5. MIXUP LOSS FOR NLP (Helps Generalization)\n",
    "# =========================================================\n",
    "\n",
    "def mixup(emb1, emb2, y1, y2, alpha=0.4):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    emb = lam * emb1 + (1 - lam) * emb2\n",
    "    y = (lam, y1, y2)\n",
    "    return emb, y\n",
    "\n",
    "def mixup_loss(logits, y_mix):\n",
    "    lam, y1, y2 = y_mix\n",
    "    return lam * F.cross_entropy(logits, y1) + (1 - lam) * F.cross_entropy(logits, y2)\n",
    "\n",
    "# =========================================================\n",
    "# 6. TRAIN ONE FOLD\n",
    "# =========================================================\n",
    "\n",
    "def train_fold(train_df, val_df, fold=1):\n",
    "\n",
    "    train_data = EmotionDataset(train_df, augment_data=True)\n",
    "    val_data = EmotionDataset(val_df, augment_data=False)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    total_steps = len(train_loader) * 4\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    print(f\"\\n======== Training FOLD {fold} ========\\n\")\n",
    "\n",
    "    for epoch in range(4):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # MIXUP 50% of the time\n",
    "            if random.random() < 0.50:\n",
    "                idx2 = torch.randperm(input_ids.size(0))\n",
    "                emb1 = input_ids.float()\n",
    "                emb2 = input_ids[idx2].float()\n",
    "                y1 = labels\n",
    "                y2 = labels[idx2]\n",
    "                emb_mix, y_mix = mixup(emb1, emb2, y1, y2)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=emb_mix.long(),\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=None\n",
    "                    )\n",
    "                    loss = mixup_loss(outputs.logits, y_mix)\n",
    "\n",
    "            else:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    # Label smoothing (epsilon = 0.1)\n",
    "                    loss = F.cross_entropy(outputs.logits, labels, label_smoothing=0.1)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # ================= VALIDATION =================\n",
    "        model.eval()\n",
    "        preds, actuals = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    ).logits\n",
    "\n",
    "                preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "                actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "        f1 = f1_score(actuals, preds, average=\"macro\")\n",
    "        acc = accuracy_score(actuals, preds)\n",
    "\n",
    "        print(f\"FOLD {fold} EPOCH {epoch+1} â†’ F1: {f1:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "        # EARLY STOPPING\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "\n",
    "            model.save_pretrained(f\"generalized_emotion_model_fold{fold}\")\n",
    "            tokenizer.save_pretrained(f\"generalized_emotion_model_fold{fold}\")\n",
    "\n",
    "            print(\"ðŸ”¥ Best model updated!\")\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= 2:\n",
    "            print(\"â›” Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best F1 for Fold {fold}: {best_f1:.4f}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. K-FOLD TRAINING\n",
    "# =========================================================\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in kf.split(df):\n",
    "    train_fold(df.iloc[train_idx], df.iloc[val_idx], fold=fold)\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e74a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: final_emotion_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# path where your best model was automatically saved earlier\n",
    "SOURCE_DIR = \"generalized_emotion_model_fold1\"   # or fold2 or fold3\n",
    "TARGET_DIR = \"final_emotion_model\"\n",
    "\n",
    "# load saved model\n",
    "model = RobertaForSequenceClassification.from_pretrained(SOURCE_DIR)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(SOURCE_DIR)\n",
    "\n",
    "# save again wherever you want\n",
    "model.save_pretrained(TARGET_DIR)\n",
    "tokenizer.save_pretrained(TARGET_DIR)\n",
    "\n",
    "print(\"Model saved to:\", TARGET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08879c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to pytorch_model.bin!\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model_dir = \"final_emotion_model\"\n",
    "\n",
    "# load safetensors\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# save as pytorch_model.bin\n",
    "model.save_pretrained(model_dir, safe_serialization=False)\n",
    "\n",
    "print(\"Converted to pytorch_model.bin!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8ffcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 18\n",
      "Emotion labels:\n",
      "0: anger\n",
      "1: anxious\n",
      "2: bored\n",
      "3: calm\n",
      "4: confused\n",
      "5: depressed\n",
      "6: excited\n",
      "7: fear\n",
      "8: frustrated\n",
      "9: grateful\n",
      "10: happy\n",
      "11: lonely\n",
      "12: motivated\n",
      "13: relieved\n",
      "14: sad\n",
      "15: stress\n",
      "16: surprised\n",
      "17: tired\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"emotion_master_200k_balanced.csv\")\n",
    "\n",
    "# Fit label encoder on the \"emotion\" column\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"emotion\"].tolist())\n",
    "\n",
    "# Print labels\n",
    "print(\"Number of labels:\", len(label_encoder.classes_))\n",
    "print(\"Emotion labels:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d92be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: tired\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD LABEL NAMES (important!)\n",
    "# -------------------------------\n",
    "id2label = {\n",
    "    0: \"anger\",\n",
    "    1: \"anxious\",\n",
    "    2: \"bored\",\n",
    "    3: \"calm\",\n",
    "    4: \"confused\",\n",
    "    5: \"depressed\",\n",
    "    6: \"excited\",\n",
    "    7: \"fear\",\n",
    "    8: \"frustrated\",\n",
    "    9: \"grateful\",\n",
    "    10: \"happy\",\n",
    "    11: \"lonely\",\n",
    "    12: \"motivated\",\n",
    "    13: \"relieved\",\n",
    "    14: \"sad\",\n",
    "    15: \"stress\",\n",
    "    16: \"surprised\",\n",
    "    17: \"tired\"\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD MODEL FROM YOUR FOLDER\n",
    "# -------------------------------\n",
    "MODEL_DIR = \"final_emotion_model\"   # <-- change if needed\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# PREDICTION FUNCTION\n",
    "# -------------------------------\n",
    "def predict_emotion(text):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=96,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    emotion = id2label[pred_id]\n",
    "\n",
    "    return emotion\n",
    "\n",
    "# -------------------------------\n",
    "# TEST\n",
    "# -------------------------------\n",
    "text = \"I am feeling very tired and stressed today\"\n",
    "prediction = predict_emotion(text)\n",
    "print(\"Predicted Emotion:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1b5233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Predicted Emotion: surprised\n",
      "Suggested Action : Interesting! Would you like recommendations?\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# ======================================================\n",
    "# 1. Load your saved model (change folder if needed)\n",
    "# ======================================================\n",
    "MODEL_DIR = \"generalized_emotion_model_fold1\"  # change fold if needed\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# ======================================================\n",
    "# 2. Emotion Labels (same as training)\n",
    "# ======================================================\n",
    "id2label = {\n",
    "    0: \"anger\",\n",
    "    1: \"anxious\",\n",
    "    2: \"bored\",\n",
    "    3: \"calm\",\n",
    "    4: \"confused\",\n",
    "    5: \"depressed\",\n",
    "    6: \"excited\",\n",
    "    7: \"fear\",\n",
    "    8: \"frustrated\",\n",
    "    9: \"grateful\",\n",
    "    10: \"happy\",\n",
    "    11: \"lonely\",\n",
    "    12: \"motivated\",\n",
    "    13: \"relieved\",\n",
    "    14: \"sad\",\n",
    "    15: \"stress\",\n",
    "    16: \"surprised\",\n",
    "    17: \"tired\"\n",
    "}\n",
    "\n",
    "# ======================================================\n",
    "# 3. Emotion â†’ Suggestion Mapping\n",
    "# ======================================================\n",
    "suggestions = {\n",
    "    \"anger\": \"Take a walk or listen to calming music.\",\n",
    "    \"anxious\": \"Try deep breathing or meditation. Nearby calm places recommended.\",\n",
    "    \"bored\": \"Explore nearby cafes or entertainment spots.\",\n",
    "    \"calm\": \"Everything looks good. Maybe enjoy a coffee nearby.\",\n",
    "    \"confused\": \"Would you like help or guidance?\",\n",
    "    \"depressed\": \"Consider talking to someone or visiting peaceful places.\",\n",
    "    \"excited\": \"Great mood! Try celebration spots or restaurants.\",\n",
    "    \"fear\": \"You may need safe locations like police station or trusted places.\",\n",
    "    \"frustrated\": \"A short break at a cafÃ© or walk can help.\",\n",
    "    \"grateful\": \"Maybe send a thank-you message or celebrate!\",\n",
    "    \"happy\": \"Enjoy! You can explore trending places nearby.\",\n",
    "    \"lonely\": \"Try social spots, cafes, or call a friend.\",\n",
    "    \"motivated\": \"Good time to start a task or visit a library/workspace.\",\n",
    "    \"relieved\": \"Relax! Maybe enjoy a peaceful place.\",\n",
    "    \"sad\": \"Try comforting places like a peaceful park or tea shop.\",\n",
    "    \"stress\": \"Consider visiting a restaurant or taking short break.\",\n",
    "    \"surprised\": \"Interesting! Would you like recommendations?\",\n",
    "    \"tired\": \"You need rest. Suggesting coffee/tea shops.\"\n",
    "}\n",
    "\n",
    "# ======================================================\n",
    "# 4. Classification + Suggestion Function\n",
    "# ======================================================\n",
    "\n",
    "def predict_emotion_and_suggestion(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred_id = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    emotion = id2label[pred_id]\n",
    "    suggestion = suggestions[emotion]\n",
    "\n",
    "    return emotion, suggestion\n",
    "\n",
    "# ======================================================\n",
    "# 5. TEST\n",
    "# ======================================================\n",
    "\n",
    "text = input(\"Enter your message: \")\n",
    "\n",
    "emotion, suggestion = predict_emotion_and_suggestion(text)\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(f\"Predicted Emotion: {emotion}\")\n",
    "print(f\"Suggested Action : {suggestion}\")\n",
    "print(\"============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff635f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: stress\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load Saved Model & Tokenizer\n",
    "# ------------------------------\n",
    "MODEL_DIR = \"final_emotion_model\"   # change if needed\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Emotion Label Mapping (18 classes)\n",
    "# ------------------------------\n",
    "id2label = {\n",
    "    0: \"anger\",\n",
    "    1: \"anxious\",\n",
    "    2: \"bored\",\n",
    "    3: \"calm\",\n",
    "    4: \"confused\",\n",
    "    5: \"depressed\",\n",
    "    6: \"excited\",\n",
    "    7: \"fear\",\n",
    "    8: \"frustrated\",\n",
    "    9: \"grateful\",\n",
    "    10: \"happy\",\n",
    "    11: \"lonely\",\n",
    "    12: \"motivated\",\n",
    "    13: \"relieved\",\n",
    "    14: \"sad\",\n",
    "    15: \"stress\",\n",
    "    16: \"surprised\",\n",
    "    17: \"tired\"\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Prediction Function\n",
    "# ------------------------------\n",
    "def predict_emotion(text):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=96\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        pred_id = torch.argmax(logits, dim=1).item()\n",
    "        pred_label = id2label[pred_id]\n",
    "\n",
    "    return pred_label\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Test It\n",
    "# ------------------------------\n",
    "test_text = \"I am feeling so stressed and worried today\"\n",
    "predicted_emotion = predict_emotion(test_text)\n",
    "\n",
    "print(\"Predicted Emotion:\", predicted_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686364f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: sad\n",
      "Suggested Places: ['quiet cafe', 'tea shop']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. Load your trained model\n",
    "# ==========================================\n",
    "MODEL_PATH = \"final_emotion_model\"   # change if needed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Emotion label mapping (your dataset)\n",
    "# ==========================================\n",
    "label_map = {\n",
    "    0: \"anger\",\n",
    "    1: \"anxious\",\n",
    "    2: \"bored\",\n",
    "    3: \"calm\",\n",
    "    4: \"confused\",\n",
    "    5: \"depressed\",\n",
    "    6: \"excited\",\n",
    "    7: \"fear\",\n",
    "    8: \"frustrated\",\n",
    "    9: \"grateful\",\n",
    "    10: \"happy\",\n",
    "    11: \"lonely\",\n",
    "    12: \"motivated\",\n",
    "    13: \"relieved\",\n",
    "    14: \"sad\",\n",
    "    15: \"stress\",\n",
    "    16: \"surprised\",\n",
    "    17: \"tired\"\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 3. Automatic place suggestions based ONLY on the model output\n",
    "# ==========================================\n",
    "emotion_to_place = {\n",
    "    \"anger\": [\"gym\", \"boxing club\", \"running track\"],\n",
    "    \"anxious\": [\"meditation center\", \"calm park\"],\n",
    "    \"bored\": [\"gaming cafe\", \"mall\", \"cinema\"],\n",
    "    \"calm\": [\"library\", \"quiet cafe\"],\n",
    "    \"confused\": [\"bookstore\", \"study cafe\"],\n",
    "    \"depressed\": [\"park\", \"quiet tea shop\"],\n",
    "    \"excited\": [\"tea shop\", \"street food\", \"hangout cafe\"],\n",
    "    \"fear\": [\"public space\", \"crowded cafe\"],\n",
    "    \"frustrated\": [\"coffee shop\", \"walkway\"],\n",
    "    \"grateful\": [\"temple\", \"family restaurant\"],\n",
    "    \"happy\": [\"restaurant\", \"ice cream shop\"],\n",
    "    \"lonely\": [\"cafe\", \"community center\"],\n",
    "    \"motivated\": [\"gym\", \"work cafe\"],\n",
    "    \"relieved\": [\"park\", \"beach side\"],\n",
    "    \"sad\": [\"quiet cafe\", \"tea shop\"],\n",
    "    \"stress\": [\"spa\", \"calm tea shop\"],\n",
    "    \"surprised\": [\"new restaurant\", \"unique cafe\"],\n",
    "    \"tired\": [\"tea shop\", \"resting area\"]\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Function: classify text and get place suggestions\n",
    "# ==========================================\n",
    "def suggest_place(user_text):\n",
    "    enc = tokenizer(\n",
    "        user_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=96\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    pred_idx = torch.argmax(logits, dim=1).item()\n",
    "    emotion = label_map[pred_idx]\n",
    "\n",
    "    # suggestions come ONLY from model prediction\n",
    "    places = emotion_to_place.get(emotion, [\"general hangout place\"])\n",
    "\n",
    "    return emotion, places\n",
    "\n",
    "# ==========================================\n",
    "# 5. Example input\n",
    "# ==========================================\n",
    "user_input = \"I am feeling very sad\"\n",
    "\n",
    "emotion, places = suggest_place(user_input)\n",
    "\n",
    "print(\"Predicted Emotion:\", emotion)\n",
    "print(\"Suggested Places:\", places)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2b32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
